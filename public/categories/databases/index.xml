<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Databases on Pivotal Engineering Journal</title>
    <link>/categories/databases/</link>
    <description>Recent content in Databases on Pivotal Engineering Journal</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 31 Mar 2016 10:57:27 +0100</lastBuildDate>
    <atom:link href="/categories/databases/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>SERIAL Datatype Performance in Greenplum Database</title>
      <link>/post/SERIAL_Datatype_Performance_in_Greenplum_Database/</link>
      <pubDate>Thu, 31 Mar 2016 10:57:27 +0100</pubDate>
      
      <guid>/post/SERIAL_Datatype_Performance_in_Greenplum_Database/</guid>
      <description>

&lt;p&gt;Storing data in a database, you quickly recognize the need for a unique identifier for every data row. Otherwise it is not always possible to join the data with other tables, or identify a specific data row. Sometimes the data itself does not provide a useful key (&amp;ldquo;natural key&amp;rdquo; or &amp;ldquo;business key&amp;rdquo;), therefore it is necessary to create a surrogate key. The tool of choice to create such technical key is the &lt;a href=&#34;http://gpdb.docs.pivotal.io/4380/ref_guide/data_types.html&#34;&gt;SERIAL datatype&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;serial-datatype:4fe49e2346d31430067989cfa55888c7&#34;&gt;SERIAL Datatype&lt;/h2&gt;

&lt;p&gt;The SERIAL datatype is a sequence, attached to a column in a table, and the next value is used as default value in case no value is specified. The following two examples show the steps outlined as SQL commands. The first example creates a table with the &lt;em&gt;id&lt;/em&gt; column as designated surrogate key:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-SQL&#34;&gt;CREATE TABLE datatable (id INTEGER NOT NULL PRIMARY KEY, data TEXT);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The second example creates a sequence, attaches the sequence to the table and adds the default value to the &lt;em&gt;id&lt;/em&gt; column.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-SQL&#34;&gt;CREATE SEQUENCE id_sequence;

ALTER SEQUENCE id_sequence OWNED BY datatable.id;

ALTER TABLE datatable
ALTER COLUMN id SET DEFAULT nextval(&#39;id_sequence&#39;);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Attaching the &lt;em&gt;id_sequence&lt;/em&gt; to the &lt;em&gt;datatable&lt;/em&gt; ensures that the sequence is dropped when the table is dropped. A sequence can only be attached to one table, but it can be used in more than one table as a default value - making it a good candidate for a globally unique key.&lt;/p&gt;

&lt;p&gt;The database provides a SERIAL datatype, which is an INTEGER with a sequence attached, and a BIGSERIAL datatype, which is a BIGINT with a sequence attached.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;Datatype&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Numeric type&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Numeric short name&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Minimum value&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Maximum value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;SERIAL&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;INTEGER&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;INT4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;-2147483648&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;+2147483647&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;BIGSERIAL&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;BIGINT&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;INT8&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;-9223372036854775808&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;9223372036854775807&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&#34;how-to-use-the-serial-datatype:4fe49e2346d31430067989cfa55888c7&#34;&gt;How to use the SERIAL Datatype&lt;/h3&gt;

&lt;p&gt;Once a table with a SERIAL datatype is created, it is easy to use this feature: just do not specify a value for this column:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-SQL&#34;&gt;serial=# INSERT INTO datatable (data) VALUES (&#39;some text&#39;);
INSERT 0 1
serial=# SELECT * FROM datatable;
 id |   data    
----+-----------
  1 | some text
(1 row)

serial=# INSERT INTO datatable (data) VALUES (&#39;some text&#39;);
INSERT 0 1
serial=# SELECT * FROM datatable;
 id |   data    
----+-----------
  1 | some text
  2 | some text
(2 rows)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The default value will ensure that the next sequence value is used.&lt;/p&gt;

&lt;h3 id=&#34;good-to-know:4fe49e2346d31430067989cfa55888c7&#34;&gt;Good to know&lt;/h3&gt;

&lt;p&gt;A sequence is never rolled back. If an INSERT is rolled back, the value used by &lt;em&gt;nextval()&lt;/em&gt; is lost, leaving a gap in the &lt;em&gt;id&lt;/em&gt; column here. This makes a sequence a good source for generating unique keys, but it does not ensure that the values are strictly sequential. Do not use this feature to generate something like &amp;ldquo;invoice numbers&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Instead of not specifying the column in the INSERT, one can also specify the keyword &lt;em&gt;DEFAULT&lt;/em&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-SQL&#34;&gt;serial=# INSERT INTO datatable (id, data) VALUES (DEFAULT, &#39;some text&#39;);
INSERT 0 1
serial=# SELECT * FROM datatable;
 id |   data
----+-----------
  1 | some text
  2 | some text
  3 | some text
(3 rows)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This might be necessary when using frameworks which generate SQL queries based on the table structure, and want to include every column of the table.&lt;/p&gt;

&lt;h2 id=&#34;serial-datatype-in-greenplum:4fe49e2346d31430067989cfa55888c7&#34;&gt;SERIAL Datatype in Greenplum&lt;/h2&gt;

&lt;p&gt;A Greenplum Database cluster uses a number of segment databases, running on shared-nothing segment servers. The segment databases share no data and no information which each other. In order to provide globally unique sequence values to the segments, the database uses a sequence server:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gpadmin   7718  0.0  0.0 361972 10448 ?    S    15:36   0:00  \_ postgres: port 5432, seqserver process
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Whenever a segment needs a value from the sequence, it connects to the sequence server and asks for the next value. You can imagine that there is some overhead involved. This overhead adds up, especially when you not only insert one or two rows, but millions of them. Fortunately, there is a tuning parameter which can be used to avoid many of the round trips to the sequence server:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-SQL&#34;&gt;ALTER SEQUENCE id_sequence CACHE 5;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Setting the cache value to &lt;em&gt;5&lt;/em&gt; will not only fetch the next value, but every segment database will fetch the next &lt;em&gt;5&lt;/em&gt; values at once, and use them for the next INSERTs. If the segment database in the end only needs 3 values, the remaining 2 are lost (remember the gaps in the sequence?). However with potentially millions of INSERTs, a few lost values are acceptable.&lt;/p&gt;

&lt;h3 id=&#34;performance-improvements-using-the-cache-parameter:4fe49e2346d31430067989cfa55888c7&#34;&gt;Performance improvements using the CACHE parameter&lt;/h3&gt;

&lt;p&gt;This raises an important question: how big is the performance improvement?&lt;/p&gt;

&lt;p&gt;The tests for the following numbers were conducted using a table with two columns (1 SERIAL; 1 floating point, filled with &lt;em&gt;1&lt;/em&gt;). For comparisation, the tests were run with different cache numbers and with a different number of rows.&lt;/p&gt;

&lt;p&gt;The first test, without SERIAL:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-SQL&#34;&gt;CREATE TABLE serial_cache (id INTEGER DEFAULT &#39;1&#39;, data DOUBLE PRECISION);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;All tests with SERIAL:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-SQL&#34;&gt;CREATE TABLE serial_cache (id SERIAL, data DOUBLE PRECISION);
ALTER SEQUENCE serial_cache_id_seq CACHE &amp;lt;n&amp;gt;;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The following &lt;em&gt;CACHE&lt;/em&gt; parameters were tested:&lt;/p&gt;

&lt;p&gt;0, 1, 2, 3, 5, 10, 20, 50, 100, 500, 1000&lt;/p&gt;

&lt;p&gt;where &lt;em&gt;y=0&lt;/em&gt; is &amp;ldquo;no SERIAL&amp;rdquo; - just an integer with a default value, but not using a sequence.&lt;/p&gt;

&lt;p&gt;The following number of rows were tested:&lt;/p&gt;

&lt;p&gt;100000, 125000, 150000, 175000, 200000&lt;/p&gt;



  &lt;figure class=&#34;img-responsive&#34;&gt;
      
          &lt;img src=&#34;/images/serial_datatype/greenplum_database/serial_by_INSERTs.jpg&#34; alt=&#34;Overall runtime for the tests&#34; class=&#34;img-responsive&#34; /&gt;
      
      
  &lt;/figure&gt;




&lt;p&gt;Looking at the first graph, it instantly becomes clear that &amp;ldquo;CACHE=1&amp;rdquo; - the default, it&amp;rsquo;s the green line - is very slow. Running this test takes almost twice as much time as running the same test with &amp;ldquo;CACHE=2&amp;rdquo; (blue line). However this graph is not broken down by the time per INSERT, it just shows the overall runtime for the tests. The next graph breaks down the numbers on a per-query basis:&lt;/p&gt;



  &lt;figure class=&#34;img-responsive&#34;&gt;
      
          &lt;img src=&#34;/images/serial_datatype/greenplum_database/serial_total_runtime.jpg&#34; alt=&#34;per-query runtime for the tests&#34; class=&#34;img-responsive&#34; /&gt;
      
      
  &lt;/figure&gt;




&lt;p&gt;Running INSERTs with &amp;ldquo;CACHE=1&amp;rdquo; is several times slower than tuning the &lt;em&gt;CACHE&lt;/em&gt; parameter. The next graph is limited to the CACHE values between &lt;em&gt;0&lt;/em&gt; (no SERIAL, just an INTEGER) and &lt;em&gt;5&lt;/em&gt;. It&amp;rsquo;s clearly visible that using a sequence adds a measurable overhead, compared to the &lt;em&gt;0&lt;/em&gt; value where no SERIAL is used.&lt;/p&gt;



  &lt;figure class=&#34;img-responsive&#34;&gt;
      
          &lt;img src=&#34;/images/serial_datatype/greenplum_database/serial_runtime_x_5.jpg&#34; alt=&#34;per-query runtime for the tests, limited to CACHE 0-5&#34; class=&#34;img-responsive&#34; /&gt;
      
      
  &lt;/figure&gt;




&lt;p&gt;It&amp;rsquo;s also interesting to see which &lt;em&gt;CACHE&lt;/em&gt; parameters make sense, and at which point they no longer improve the performance. The following graph shows all &lt;em&gt;CACHE&lt;/em&gt; parameters, but limits the view to &lt;em&gt;y=20μs&lt;/em&gt;.&lt;/p&gt;



  &lt;figure class=&#34;img-responsive&#34;&gt;
      
          &lt;img src=&#34;/images/serial_datatype/greenplum_database/serial_runtime_y_20.jpg&#34; alt=&#34;per-query runtime for the tests, limited to y=20&#34; class=&#34;img-responsive&#34; /&gt;
      
      
  &lt;/figure&gt;




&lt;p&gt;In this graph, the improvements look big, even between &lt;em&gt;CACHE=500&lt;/em&gt; and &lt;em&gt;CACHE=1000&lt;/em&gt;. However compared in the big picture, &lt;em&gt;CACHE=1&lt;/em&gt; needs around &lt;em&gt;900μs&lt;/em&gt;, &lt;em&gt;CACHE=15&lt;/em&gt; already brings down the query runtime to &lt;em&gt;10μs&lt;/em&gt;, and &lt;em&gt;CACHE=1000&lt;/em&gt; only improves this down to around &lt;em&gt;3μs&lt;/em&gt;. A query without a sequence runs in around &lt;em&gt;1μs&lt;/em&gt;, for comparisation.&lt;/p&gt;

&lt;h2 id=&#34;serial-datatype-in-postgresql:4fe49e2346d31430067989cfa55888c7&#34;&gt;SERIAL Datatype in PostgreSQL&lt;/h2&gt;

&lt;p&gt;How does this compare to PostgreSQL, running the same tests?&lt;/p&gt;

&lt;p&gt;

  &lt;figure class=&#34;img-responsive&#34;&gt;
      
          &lt;img src=&#34;/images/serial_datatype/postgresql/serial_total_runtime.jpg&#34; alt=&#34;per-query runtime for the tests&#34; class=&#34;img-responsive&#34; /&gt;
      
      
  &lt;/figure&gt;


&lt;/p&gt;

&lt;p&gt;The query runtime averages around &lt;em&gt;1.5μs&lt;/em&gt;, just the start with &lt;em&gt;CACHE=0&lt;/em&gt; comes down to around  &lt;em&gt;1.1μs&lt;/em&gt;. There is no huge performance loss when using a sequence, compared to the spike in Greenplum Database. Looking into the details:&lt;/p&gt;

&lt;p&gt;

  &lt;figure class=&#34;img-responsive&#34;&gt;
      
          &lt;img src=&#34;/images/serial_datatype/postgresql/serial_runtime_x_5.jpg&#34; alt=&#34;per-query runtime for the tests, limited to CACHE 0-5&#34; class=&#34;img-responsive&#34; /&gt;
      
      
  &lt;/figure&gt;





  &lt;figure class=&#34;img-responsive&#34;&gt;
      
          &lt;img src=&#34;/images/serial_datatype/postgresql/serial_runtime_y_2.jpg&#34; alt=&#34;per-query runtime for the tests, limited to y=2&#34; class=&#34;img-responsive&#34; /&gt;
      
      
  &lt;/figure&gt;


&lt;/p&gt;

&lt;p&gt;The numbers show that there is a small performance loss when using a sequence. The graph goes up for &lt;em&gt;CACHE=1&lt;/em&gt;, comes back down for &lt;em&gt;CACHE=2&lt;/em&gt; and &lt;em&gt;CACHE=3&lt;/em&gt;, and no further improvements for higher &lt;em&gt;CACHE&lt;/em&gt; values.&lt;/p&gt;

&lt;h2 id=&#34;conclusion:4fe49e2346d31430067989cfa55888c7&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;The INSERT performance in Greenplum Database improves greatly, if a sequence uses at least &lt;em&gt;CACHE=5&lt;/em&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Current TransactionID in Greenplum Database</title>
      <link>/post/Current_TransactionID_in_Greenplum_Database/</link>
      <pubDate>Sat, 27 Feb 2016 01:58:42 +0100</pubDate>
      
      <guid>/post/Current_TransactionID_in_Greenplum_Database/</guid>
      <description>

&lt;p&gt;Recently, during a training, a colleague asked me how to find out the current TransactionID in a &lt;a href=&#34;http://greenplum.org/&#34;&gt;Greenplum Database&lt;/a&gt; system. This piece of information is important in order to find out if a &lt;a href=&#34;http://gpdb.docs.pivotal.io/4370/ref_guide/sql_commands/VACUUM.html&#34;&gt;VACUUM&lt;/a&gt; run is required on a table.&lt;/p&gt;

&lt;h3 id=&#34;postgresql:73a5860bb147c976ac1f5e05e53b8dca&#34;&gt;PostgreSQL&lt;/h3&gt;

&lt;p&gt;In &lt;a href=&#34;http://www.postgresql.org/&#34;&gt;PostgreSQL&lt;/a&gt; (Greenplum Database is a PostgreSQL fork) this is easy:&lt;/p&gt;

&lt;p&gt;Current TransactionID in PostgreSQL:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;postgres=&amp;gt; SELECT txid_current();
 txid_current 
--------------
          816
(1 row)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;greenplum:73a5860bb147c976ac1f5e05e53b8dca&#34;&gt;Greenplum&lt;/h3&gt;

&lt;p&gt;Unfortunately, the &lt;a href=&#34;https://github.com/greenplum-db/gpdb&#34;&gt;merge process&lt;/a&gt; with PostgreSQL has not yet merged in this function. Therefore things get a bit more complicated, and a workaround is required. The system table &amp;lsquo;&lt;em&gt;pg_locks&lt;/em&gt;&amp;rsquo; shows, among other information, the TransactionID. Selecting from this table will show at least two records: one &lt;em&gt;AccessShareLock&lt;/em&gt; on &amp;lsquo;&lt;em&gt;pg_locks&lt;/em&gt;&amp;rsquo; itself, and one &lt;em&gt;ExclusiveLock&lt;/em&gt; on the current TransactionID.&lt;/p&gt;

&lt;p&gt;Current TransactionID in Greenplum:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;postgres=&amp;gt; SELECT transactionid FROM pg_locks
WHERE pid = pg_backend_pid() AND locktype = &#39;transactionid&#39; AND mode = &#39;ExclusiveLock&#39; AND granted = &#39;t&#39;;
 transactionid
---------------
          1325
(1 row)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In most cases it is enough to just limit the results by &amp;lsquo;&lt;em&gt;pid&lt;/em&gt;&amp;rsquo; and &amp;lsquo;&lt;em&gt;locktype&lt;/em&gt;&amp;rsquo;.&lt;/p&gt;

&lt;p&gt;Current TransactionID in Greenplum (shorter):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;postgres=&amp;gt; SELECT transactionid FROM pg_locks
WHERE pid = pg_backend_pid() AND locktype = &#39;transactionid&#39;;
 transactionid
---------------
          1325
(1 row)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;On the downside, this query is consuming yet another TransactionID, if run in a separate transaction.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Pivotal Data Open Source in 2016: community, community, community!
</title>
      <link>/post/data-open-source-community-2016/</link>
      <pubDate>Fri, 29 Jan 2016 07:00:00 -0800</pubDate>
      
      <guid>/post/data-open-source-community-2016/</guid>
      <description>&lt;p&gt;When it comes to Open Source, it wouldn’t be too much of a stretch to say that 2015 was a &lt;a href=&#34;https://www.youtube.com/watch?v=XxXgAPANmWw&#34;&gt;defining year&lt;/a&gt; for Pivotal Data. Almost a year ago, on February 17th 2015, we &lt;a href=&#34;https://www.youtube.com/watch?v=dTG5TAbzUAY&#34;&gt;announced&lt;/a&gt; our overarching strategy to complete the transition towards Pivotal becoming a quintessential Open Source company. We called it a New Approach to Big Data and we gave ourselves slightly less than a year to deliver on that promise. What happened in the rest of 2015 was nothing short of amazing. In 7 month from strategy to execution we open sourced our entire portfolio of Data products (Pivotal Gemfire, Pivotal HAWQ and Pivotal Greenplum) and jump started an industry collaboration on standardizing  Big Data platforms via &lt;a href=&#34;http://odpi.org&#34;&gt;ODPi&lt;/a&gt; collaborative project. But most importantly we put a very strong foundation in place to start growing amazing communities around our newly minted open source projects.&lt;/p&gt;

&lt;p&gt;It is on that foundation that we intend to build in 2016. And we’re kicking it off in style.&lt;/p&gt;

&lt;p&gt;In the month of January alone we already had 6 virtual community events showcasing contributions coming from a wide spectrum of community members. We plan to have a robust schedule of these events for the rest of 2016 and if you are interested in presenting at one of those please let us know – we’d love to feature your work. Of course, while virtual events are fun and they have an advantage of allowing participants not to wear pants (trust me – I know from experience) nothing beats the excitement of hanging out with your fellow hackers face to face.  To that end, we will be hosting up to a dozen events a month  (check out our &lt;a href=&#34;https://calendar.google.com/calendar/embed?src=pivotal.io_u8kgvuahjkboh1gnfhv5ts2v9c%40group.calendar.google.com&amp;amp;ctz=America/Los_Angeles&#34;&gt;Pivotal Data Communities Public Calendar&lt;/a&gt;) and it goes without saying that we would love if you would love to participate in one of our events, or consider hosting an event and inviting us to present. . In addition to meetups our first conference style gathering of users and hackers is scheduled to be a &lt;a href=&#34;http://geodesummit.com/&#34;&gt;Geode Summit&lt;/a&gt;. Make sure to register now, but also keep an eye on future announcements. We definitely are thinking of similar events for our other communities. Finally, we feel that a little bit of a competitive spirit is always a good thing especially among Open Source hackers. In 2015 we ran our first contest around &lt;a href=&#34;http://ambitious-apps.devpost.com/&#34;&gt;Apache Geode (incubating)&lt;/a&gt; and had one lucky team take home not only the glory of winning, but also the fortune of Apple Watch &amp;amp; Apache Geode hoodie. For 2016 we were kicking around this idea of a contest around creating a true self conscious AI, but since Google &lt;a href=&#34;http://www.wired.com/2016/01/in-a-huge-breakthrough-googles-ai-beats-a-top-player-at-the-game-of-go/&#34;&gt;beat&lt;/a&gt; us all to it we’re open to other suggestions.&lt;/p&gt;

&lt;p&gt;At this point, I can hear you asking “but Roman, how do I get a hold of you guys to discuss this awesome idea of mine?”. Nothing could be simpler. First of all, all of our open source projects have open community mailing lists. Dropping a note to a community mailing list is the easiest way to get the conversation going and you can find instructions on how to subscribe, post and browse archives at each project’s website: &lt;a href=&#34;http://geode.incubator.apache.org/community/&#34;&gt;Apache Geode (incubating)&lt;/a&gt;, &lt;a href=&#34;http://hawq.incubator.apache.org/#mailing-lists&#34;&gt;Apache HAWQ (incubating)&lt;/a&gt;, &lt;a href=&#34;http://madlib.incubator.apache.org/community.html&#34;&gt;Apache MADlib (incubating)&lt;/a&gt;, &lt;a href=&#34;http://greenplum.org/#mailing-lists&#34;&gt;Greenplum Database&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We are also very likely to run into each other at one of the big, established gatherings of open source developers. As I am writing this, a whole bunch of us are converging on Brussels to kick off 2016 at the grand daddy of all open source conferences: &lt;a href=&#34;http://fosdem.org/&#34;&gt;FOSDEM&lt;/a&gt;. At FOSDEM you can find us at &lt;a href=&#34;http://fosdem2016.pgconf.eu/&#34;&gt;FOSDEM PGDay&lt;/a&gt;, &lt;a href=&#34;http://www.apache.org/&#34;&gt;Apache Software Foundation&lt;/a&gt; booth and most importantly a &lt;a href=&#34;https://fosdem.org/2016/schedule/track/hpc,_big_data_and_data_science/&#34;&gt;HPC, Big Data and Data Science devroom&lt;/a&gt;. To show our appreciation for all of our community members we are also hosting a dinner on Saturday, Feb 30 at 7pm in this lovely place called &lt;a href=&#34;http://www.mirabelle.be/&#34;&gt;La Mirabelle&lt;/a&gt;. There we will be toasting our latest addition to Pivotal Open Source family: &lt;a href=&#34;https://github.com/greenplum-db/gporca&#34;&gt;Orca&lt;/a&gt;. Orca is a &lt;a href=&#34;http://pivotal.io/big-data/white-paper/orca-a-modular-query-optimizer-architecture-for-big-data&#34;&gt;modular query optimizer for big data&lt;/a&gt; and we are really excited to share this breakthrough research in databases with all of you.&lt;/p&gt;

&lt;p&gt;With that, I really hope see all of you soon. Drop by. Say hi. It’ll be a fun 2016!&lt;/p&gt;

&lt;p&gt;Roman Shaposhnik,
Director of Open Source Strategy at Pivotal&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>GPORCA, A Modular Query Optimizer, Is Now Open-Source</title>
      <link>/post/gporca-open-source/</link>
      <pubDate>Thu, 28 Jan 2016 07:00:00 -0800</pubDate>
      
      <guid>/post/gporca-open-source/</guid>
      <description>

&lt;h1 id=&#34;gporca-a-cost-based-query-optimizer-is-now-open-source:c0657d5419a8e9dbf76f780caab6c155&#34;&gt;GPORCA, A Cost-Based Query Optimizer, Is Now Open Source&lt;/h1&gt;

&lt;p&gt;Pick your favorite Big Data statistic; we all know that the growth of data has been astounding. What hasn’t kept up is our ability to process this data.  Even the newest big data technologies such as &lt;a href=&#34;http://spark.apache.org/&#34;&gt;Apache Spark™&lt;/a&gt; are using &lt;a href=&#34;https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html&#34;&gt;rule-based&lt;/a&gt; approaches to query optimization, which often miss potential faster execution plans.&lt;/p&gt;

&lt;p&gt;This is why the Pivotal R&amp;amp;D team poured years of work into developing a new query optimizer called GPORCA. Improving the intelligence of query planning was a missed chance in database engineering with huge opportunity for further improving performance.&lt;/p&gt;

&lt;p&gt;The truth is, the industry can’t wait for a war over performance to play out between the many new data technologies. By collaborating with leading developers, it will be possible to benefit all users of different database technologies and advance innovation in the industry faster. That is why Pivotal is announcing that the GPORCA query optimizer is now open source and available under the &lt;a href=&#34;http://www.apache.org/licenses/LICENSE-2.0&#34;&gt;Apache License v2.0&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Currently it is possible to run GPORCA with open source projects &lt;a href=&#34;http://greenplum.org/&#34;&gt;Greenplum Database&lt;/a&gt; and &lt;a href=&#34;http://hawq.incubator.apache.org/&#34;&gt;Apache HAWQ (incubating)&lt;/a&gt;. We call on the database engineering community to help us increase the portability of GPORCA to work with even more data management platforms.&lt;/p&gt;

&lt;h1 id=&#34;why-should-i-care:c0657d5419a8e9dbf76f780caab6c155&#34;&gt;Why Should I Care?&lt;/h1&gt;

&lt;p&gt;Historically, every database has shipped with its own optimizer. That means every software developer spent valuable R&amp;amp;D cycles building one, and maintaining it. This is not a scalable solution, nor does it foster collaborative research.&lt;/p&gt;

&lt;p&gt;GPORCA is built as an external plugin, which makes it the perfect test bed for database research that can benefit a wide variety of databases.&lt;/p&gt;

&lt;p&gt;Furthermore, GPORCA isn’t just a tool for research and development. It is an enterprise-grade, query optimizer that is handling some of the most demanding SQL workloads at truly Big Data scale.&lt;/p&gt;

&lt;h1 id=&#34;what-is-gporca:c0657d5419a8e9dbf76f780caab6c155&#34;&gt;What Is GPORCA?&lt;/h1&gt;

&lt;p&gt;GPORCA is a modular, cost-based query optimizer based on 30 years of &lt;a href=&#34;https://d1fto35gcfffzn.cloudfront.net/big-data/white-paper/SIGMODHAWQAdvantages.pdf&#34;&gt;database research&lt;/a&gt;. Simply put, it takes in a parsed query statement and returns what it considers to be the fastest execution plan for the database. It combines the query with metadata (e.g. statistics, schemas, etc.) and information about the database cluster to generate the execution plan. GPORCA is portable and modular because it can work with more than one database and can easily support new database operators.&lt;/p&gt;

&lt;h2 id=&#34;what-can-gporca-do:c0657d5419a8e9dbf76f780caab6c155&#34;&gt;What Can GPORCA Do?&lt;/h2&gt;

&lt;p&gt;At Pivotal, we are constantly benchmarking our database products to ensure they are getting faster. Currently GPORCA is achieving an overall 5X improvement compared to Pivotal Greenplum’s legacy planner across all 99 of the industry’s most trusted benchmark queries. Some of the very complex queries are up to 1000x faster!&lt;/p&gt;



  &lt;figure class=&#34;img-responsive&#34;&gt;
      
          &lt;img src=&#34;/images/gporca/gporcaPerf.png&#34;  class=&#34;img-responsive&#34; /&gt;
      
      
  &lt;/figure&gt;




&lt;p&gt;Three features of GPORCA are primarily responsible for these performance gains: Dynamic Partition Elimination, SubQuery Unnesting, and Common Table Expression. To read more about these three features, please check &lt;a href=&#34;https://blog.pivotal.io/big-data-pivotal/products/greenplum-database-adds-the-pivotal-query-optimizer&#34;&gt;this blog post&lt;/a&gt; published in 2015.&lt;/p&gt;

&lt;p&gt;GPORCA achieves its portability and modularity by running outside the core database system. Furthermore, the primary optimization techniques are componentized, allowing new operators, transformation, statistical/cost models and other techniques to be added with ease.&lt;/p&gt;



  &lt;figure class=&#34;img-responsive&#34;&gt;
      
          &lt;img src=&#34;/images/gporca/gporcaArch.png&#34;  class=&#34;img-responsive&#34; /&gt;
      
      
  &lt;/figure&gt;




&lt;p&gt;For a more technical deep-dive into GPORCA, we encourage you to take a look at the &lt;a href=&#34;http://pivotal.io/big-data/white-paper/orca-a-modular-query-optimizer-architecture-for-big-data&#34;&gt;white paper&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&#34;what-can-t-gporca-do:c0657d5419a8e9dbf76f780caab6c155&#34;&gt;What Can’t GPORCA Do?&lt;/h1&gt;

&lt;p&gt;As are many things in life, writing a modular query optimizer is about finding balance.  The original design for GPORCA optimized for analytical queries on data warehouses.  We focused on queries that typically would take a few hours, and made them run in a few minutes. For these long running queries, the time needed to compute an optimal plan is small compared with the duration of the query itself.  However, for shorter queries, the time needed to find an optimal plan becomes more important for overall execution time, so this is an area that needs future development effort. This is all about finding balance and bottlenecks.&lt;/p&gt;

&lt;p&gt;Furthermore, GPORCA is not feature complete compared to the current planner in GPDB and HAWQ, which is a modified version of the Postgres planner designed to work on MPP databases. When GPORCA encounters an operator it does not currently support, it falls back to the legacy planner.&lt;/p&gt;

&lt;h1 id=&#34;where-is-gporca-going:c0657d5419a8e9dbf76f780caab6c155&#34;&gt;Where Is GPORCA Going?&lt;/h1&gt;

&lt;p&gt;GPORCA needs to become feature complete, matching PostgreSQL’s current support and beyond. Support for external parameters, cubes, multiple grouping sets, inverse distribution functions, ordered aggregates, and indexed expressions is all on the GPORCA roadmap.&lt;/p&gt;

&lt;p&gt;With parity and better performance for shorter running queries comes the ultimate goal of GPORCA. We hope one day, GPORCA is the defacto query optimizer in PostgreSQL.&lt;/p&gt;

&lt;h1 id=&#34;why-does-gporca-need-you:c0657d5419a8e9dbf76f780caab6c155&#34;&gt;Why Does GPORCA  Need YOU?&lt;/h1&gt;

&lt;p&gt;GPORCA is organized as a sub-project of &lt;a href=&#34;http://greenplum.org/&#34;&gt;Greenplum Database&lt;/a&gt;. However, GPORCA is designed to be database agnostic. The contributor team here at Pivotal primarily uses the Greenplum Database as a test bed and to ensure that the GPORCA API is easy to implement.&lt;/p&gt;

&lt;p&gt;Here’s how to  get started with GPORCA and the GPORCA community:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Head over to &lt;a href=&#34;https://github.com/greenplum-db/gpos&#34;&gt;github&lt;/a&gt; and check out the GPOS README. After that, you will need to compile both GPORCA and GPDB.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Once GPDB is up and running with GPORCA, &lt;code&gt;set optimizer=on;&lt;/code&gt; will turn on GPORCA.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Run explain analyze with GPORCA on and off to see if GPORCA calculates a better query plan.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Subscribe to the GPDB developer mailing list: &lt;a href=&#34;mailto:gpdb-users+subscribe@greenplum.org&#34;&gt;gpdb-users+subscribe@greenplum.org&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Get involved in the discussions on github.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Collaborate on stories in &lt;a href=&#34;https://www.pivotaltracker.com/n/projects/1523545&#34;&gt;Pivotal Tracker&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Sign the &lt;a href=&#34;https://github.com/greenplum-db/greenplum-db.github.io/wiki/Greenplum-Database-project-contributions-FAQ#q-do-i-need-to-sign-anything-in-order-to-contribute-code-to-greenplum-database&#34;&gt;ICLA/CCLA&lt;/a&gt; and start sending us &lt;a href=&#34;https://github.com/greenplum-db/greenplum-db.github.io/wiki/Merging-Pull-Requests&#34;&gt;pull requests&lt;/a&gt;!&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
  </channel>
</rss>
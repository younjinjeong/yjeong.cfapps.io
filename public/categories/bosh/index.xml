<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Bosh on Pivotal Engineering Journal</title>
    <link>/categories/bosh/</link>
    <description>Recent content in Bosh on Pivotal Engineering Journal</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 24 Oct 2015 13:52:48 -0700</lastBuildDate>
    <atom:link href="/categories/bosh/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>The World&#39;s Smallest Concourse CI Server</title>
      <link>/post/worlds-smallest-concourse-server/</link>
      <pubDate>Sat, 24 Oct 2015 13:52:48 -0700</pubDate>
      
      <guid>/post/worlds-smallest-concourse-server/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Continuous_integration&#34;&gt;Continuous Integration&lt;/a&gt;
(CI) is often used in conjunction with test-driven development (TDD);
however, CI servers often
bring their own set of challenges: they are usually &amp;ldquo;snowflakes&amp;rdquo;,
uniquely configured machines that are difficult to upgrade, re-configure, or
re-install. &lt;sup&gt;&lt;a href=&#34;#snowflakes&#34;&gt;[snowflakes]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;In this blog post, we describe deploying a publicly-accessible, lean
(1GB RAM, 1 vCPU, 15GB disk)
&lt;a href=&#34;http://concourse.ci/&#34;&gt;Concourse&lt;/a&gt; CI server using a 350-line manifest.
Upgrades/re-configurations/re-installs are as simple as editing a file
and typing one command (&lt;em&gt;bosh-init&lt;/em&gt;).&lt;/p&gt;

&lt;p&gt;We must confess to sleight-of-hand: although we describe deploying a CI
server, the worker is too lean to run any but the smallest tests.
&lt;strong&gt;The deployed CI Server we describe can&amp;rsquo;t run large tests&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;We will address that shortcoming in a future blog post where we describe
the manual provisioning of local workers. The process isn&amp;rsquo;t difficult, but
including it would have made the blog post undesirably long.&lt;/p&gt;

&lt;h2 id=&#34;0-set-up-the-concourse-server:250d623a31e5e446b416d6f4fb2feaa8&#34;&gt;0. Set Up the Concourse Server&lt;/h2&gt;

&lt;p&gt;In the following steps, we demonstrate creating a Concourse
CI server, &lt;a href=&#34;https://ci.blabbertabber.com/&#34;&gt;ci.blabbertabber.com&lt;/a&gt;
for our open source Android project,
&lt;a href=&#34;https://github.com/blabbertabber/blabbertabber&#34;&gt;BlabberTabber&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;0-0-prepare-an-aws-account:250d623a31e5e446b416d6f4fb2feaa8&#34;&gt;0.0 Prepare an AWS Account&lt;/h3&gt;

&lt;p&gt;We follow the bosh.io &lt;a href=&#34;http://bosh.io/docs/init-aws.html#prepare-aws&#34;&gt;instructions&lt;/a&gt;
&lt;sup&gt;&lt;a href=&#34;#aws_tooling&#34;&gt;[AWS tooling]&lt;/a&gt;&lt;/sup&gt;
to prepare our AWS account.&lt;/p&gt;

&lt;p&gt;After we have completed this step, we have the information
we need to populate our BOSH deployment&amp;rsquo;s manifest:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Access Key ID, e.g. AKIAxxxxxxxxxxxxxxxx&lt;/li&gt;
&lt;li&gt;Secret Access Key, e.g. 0+B1XW6VVxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx&lt;/li&gt;
&lt;li&gt;Availability Zone, e.g. us-east-1a&lt;/li&gt;
&lt;li&gt;Region, e.g. us-east-1&lt;/li&gt;
&lt;li&gt;Elastic IP, e.g. 52.23.10.10&lt;/li&gt;
&lt;li&gt;Subnet ID, e.g. subnet-1c90ef6b&lt;/li&gt;
&lt;li&gt;Key Pair Name, e.g. bosh, aws_nono&lt;/li&gt;
&lt;li&gt;Key Pair Path, e.g. ~/my-bosh/bosh.pem, ~/.ssh/aws_nono.pem&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;0-1-create-the-concourse-security-group:250d623a31e5e446b416d6f4fb2feaa8&#34;&gt;0.1 Create the &lt;em&gt;concourse&lt;/em&gt; Security Group&lt;/h3&gt;

&lt;p&gt;Although we created an AWS Security Group in the previous step, it doesn&amp;rsquo;t
suit our purposes&amp;mdash;we need to open the ports for HTTP (80) and HTTPS (443).
We create a &lt;em&gt;concourse&lt;/em&gt; Security Group via the Amazon AWS Console:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;VPC &amp;rarr; Security Groups&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;click &lt;strong&gt;Create Security Group&lt;/strong&gt;

&lt;ul&gt;
&lt;li&gt;Name tag: &lt;strong&gt;concourse&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Description: &lt;strong&gt;Rules for accessing Concourse CI server&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;VPC: &lt;em&gt;select the VPC in which you created in the previous step&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;click &lt;strong&gt;Yes, Create&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;click &lt;strong&gt;Inbound Rules&lt;/strong&gt; tab&lt;/li&gt;
&lt;li&gt;click &lt;strong&gt;Edit&lt;/strong&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;add the following rules:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;Type of Traffic&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Protocol&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Port Range&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Source IP CIDR&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Notes&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;SSH (22)&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;TCP (6)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;22&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0.0.0/0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;debugging, agents&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;HTTP (80)&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;TCP (6)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;80&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0.0.0/0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;redirect&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;HTTPS (443)&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;TCP (6)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;443&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0.0.0/0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;web&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Custom TCP Rule&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;TCP (6)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2222&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0.0.0/0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;agents&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Custom TCP Rule&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;TCP (6)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6868&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0.0.0/0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;bosh-init&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;click &lt;strong&gt;Save&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;0-2-obtain-ssl-certificates:250d623a31e5e446b416d6f4fb2feaa8&#34;&gt;0.2 Obtain SSL Certificates&lt;/h3&gt;

&lt;p&gt;We decide to use HTTPS to communicate with our Concourse server, for we will
need to authenticate against the webserver when we configure our CI
(when we transmit our credentials over the Internet we want them to be encrypted).&lt;/p&gt;

&lt;p&gt;We purchase valid SSL certificates for our server &lt;sup&gt;&lt;a href=&#34;#lets_encrypt&#34;&gt;[Let&amp;rsquo;s Encrypt]&lt;/a&gt;&lt;/sup&gt; .
Using a self-signed certificate is also an option.&lt;/p&gt;

&lt;p&gt;We use the following command to create our key and CSR. Note that you
should substitute your information where appropriate, especially for the
CN (Common Name), i.e. don&amp;rsquo;t use &lt;em&gt;ci.blabbertabber.com&lt;/em&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;openssl req -new \
  -keyout ci.blabbertabber.com.key \
  -out ci.blabbertabber.com.csr \
  -newkey rsa:4096 -sha256 -nodes \
  -subj &#39;/C=US/ST=California/L=San Francisco/O=blabbertabber.com/OU=/CN=ci.blabbertabber.com/emailAddress=brian.cunnie@gmail.com&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We submit the CSR to our vendor, authorize the issuance of the certificate,
and receive our certificate, which we will place in our manifest (along with
the key and the CA certificate chain).&lt;/p&gt;

&lt;p&gt;We configure a DNS A record for our concourse server to point to our AWS Elastic IP.
We have the following line in our blabbertabber.com zone file:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ci.blabbertabber.com. A 52.23.10.10
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;0-3-create-private-ssh-key-for-remote-workers:250d623a31e5e446b416d6f4fb2feaa8&#34;&gt;0.3 Create Private SSH Key for Remote workers&lt;/h3&gt;

&lt;p&gt;We create a private ssh key for our remote worker:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ssh-keygen -P &#39;&#39;  -f ~/.ssh/worker_key
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We will use the public key in the next step, when we create our BOSH manifest.&lt;/p&gt;

&lt;h3 id=&#34;0-4-create-bosh-manifest:250d623a31e5e446b416d6f4fb2feaa8&#34;&gt;0.4 Create BOSH Manifest&lt;/h3&gt;

&lt;p&gt;We create the BOSH manifest for our Concourse server by doing the following:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;download the redacted (passwords &amp;amp; keys removed) BOSH &lt;a href=&#34;https://github.com/blabbertabber/blabbertabber/blob/develop/ci/concourse-aws.yml&#34;&gt;manifest&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;open it in an editor&lt;/li&gt;
&lt;li&gt;search for every occurrence of &lt;em&gt;FIXME&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;update the field described by the &lt;em&gt;FIXME&lt;/em&gt;, e.g. update the IP address,
set the password, set the Subnet ID, etc&amp;hellip;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For those interested, our sample Concourse manifest was derived from
Concourse&amp;rsquo;s official sample &lt;em&gt;bosh-init&lt;/em&gt;
&lt;a href=&#34;https://github.com/concourse/concourse/blob/master/manifests/bosh-init-aws.yml&#34;&gt;manifest&lt;/a&gt; and modified as follows:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;added an nginx release to provide SSL termination (i.e. HTTPS)
while bypassing the need for an ELB ($219.14/year &lt;sup&gt;&lt;a href=&#34;#ELB-pricing&#34;&gt;[ELB-pricing]&lt;/a&gt;&lt;/sup&gt; ).
This was also why we enabled ports 80 and 443 in our AWS Security Group.&lt;/li&gt;
&lt;li&gt;added a postgres job and configured our Concourse server to use that instead of Amazon RDS in order
to eliminate RDS charges, but we suspect the savings to be insignificant.&lt;/li&gt;
&lt;li&gt;configured the web interface to be publicly-viewable but require authorization to make changes&lt;/li&gt;
&lt;li&gt;added our remote worker&amp;rsquo;s public key to &lt;em&gt;jobs.properties.tsa.authorized_keys&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;0-5-deploy-the-concourse-server:250d623a31e5e446b416d6f4fb2feaa8&#34;&gt;0.5 Deploy the Concourse Server&lt;/h3&gt;

&lt;p&gt;We install &lt;em&gt;bosh-init&lt;/em&gt; by following these &lt;a href=&#34;https://bosh.io/docs/install-bosh-init.html&#34;&gt;instructions&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We use &lt;em&gt;bosh-init&lt;/em&gt; to deploy Concourse using the manifest
we created in the previous step. In the following example, our manifest is
named &lt;em&gt;concourse-aws.yml&lt;/em&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bosh-init deploy concourse-aws.yml
  ...
  Finished deploying (00:12:15)

  Stopping registry... Finished (00:00:00)
  Cleaning up rendered CPI jobs... Finished (00:00:00)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A deployment takes ~12 minutes.This &lt;a href=&#34;https://gist.github.com/cunnie/088794c9566a707aed71&#34;&gt;gist&lt;/a&gt; contains
the complete output of the &lt;em&gt;bosh-init&lt;/em&gt; deployment.&lt;/p&gt;

&lt;h3 id=&#34;0-6-verify-deployment-and-download-concourse-cli:250d623a31e5e446b416d6f4fb2feaa8&#34;&gt;0.6 Verify Deployment and Download Concourse CLI&lt;/h3&gt;

&lt;p&gt;We browse to &lt;a href=&#34;https://ci.blabbertabber.com&#34;&gt;https://ci.blabbertabber.com&lt;/a&gt;.&lt;/p&gt;



  &lt;figure class=&#34;img-responsive&#34;&gt;
      
          &lt;img src=&#34;/images/concourse-000/no_pipelines.png&#34;  class=&#34;img-responsive&#34; /&gt;
      
      
  &lt;/figure&gt;




&lt;p&gt;We download the &lt;code&gt;fly&lt;/code&gt; CLI by clicking on the Apple icon (assuming that your workstation is an OS X machine) and move it into place:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;install ~/Downloads/fly /usr/local/bin
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;0-7-create-hello-world-concourse-job:250d623a31e5e446b416d6f4fb2feaa8&#34;&gt;0.7 Create &lt;em&gt;Hello World&lt;/em&gt; Concourse job&lt;/h3&gt;

&lt;p&gt;We follow Concourse&amp;rsquo;s &lt;a href=&#34;http://concourse.ci/getting-started.html&#34;&gt;Getting Started&lt;/a&gt;
instructions to create our first pipeline. We
add &lt;code&gt;tags [ &amp;quot;micro&amp;quot; ]&lt;/code&gt; to the sample Concourse pipeline
so that the job is run on our &amp;ldquo;micro&amp;rdquo; worker (in
our BOSH manifest, we tag the worker that
is colocated on our &lt;em&gt;t2.micro&lt;/em&gt; Concourse
server &amp;ldquo;micro&amp;rdquo; so that we can steer small jobs
to it).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cat &amp;gt; hello-world.yml &amp;lt;&amp;lt;EOF
jobs:
- name: hello-world
  plan:
  - task: say-hello
    config:
      platform: linux
      image: &amp;quot;docker:///ubuntu&amp;quot;
      tags: [ &amp;quot;micro&amp;quot; ]
      run:
        path: echo
        args: [&amp;quot;Hello, world!&amp;quot;]
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We configure the pipeline (remember to substitute the username and password in the manifest,
&lt;em&gt;jobs.properties.atc.basic_auth_username&lt;/em&gt; and &lt;em&gt;jobs.properties.atc.basic_auth_password&lt;/em&gt;,
for &amp;ldquo;user:password&amp;rdquo; below):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fly -t &amp;quot;https://user:password@ci.blabbertabber.com&amp;quot; set-pipeline -p really-cool-pipeline -c hello-world.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can see the gist of the output &lt;a href=&#34;https://gist.github.com/cunnie/a3a125e9069e493ef8ca&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Type &lt;strong&gt;y&lt;/strong&gt; when prompted to apply the configuration.&lt;/p&gt;

&lt;h3 id=&#34;0-8-browse-to-concourse-and-kick-off-job:250d623a31e5e446b416d6f4fb2feaa8&#34;&gt;0.8 Browse to Concourse and Kick off Job&lt;/h3&gt;

&lt;p&gt;Refresh &lt;a href=&#34;https://ci.blabbertabber.com&#34;&gt;https://ci.blabbertabber.com&lt;/a&gt; to
see our newly-created pipeline:&lt;/p&gt;



  &lt;figure class=&#34;img-responsive&#34;&gt;
      
          &lt;img src=&#34;/images/concourse-000/new_pipeline.png&#34;  class=&#34;img-responsive&#34; /&gt;
      
      
  &lt;/figure&gt;




&lt;p&gt;Next we unpause the job&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;click the &amp;ldquo;&amp;equiv;&amp;rdquo; (hamburger) in the upper left hand corner&lt;/li&gt;
&lt;li&gt;click the &amp;ldquo;&amp;#x25b6;&amp;rdquo; (play button) that appears below the hamburger.
This will un-pause our pipeline and allow builds to run.&lt;/li&gt;
&lt;li&gt;click &lt;strong&gt;Log in with Basic Auth&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;authenticate with the &lt;em&gt;atc&lt;/em&gt;&amp;rsquo;s account and password (these can be found in the manifest,
&lt;em&gt;jobs.properties.atc.basic_auth_username&lt;/em&gt; and &lt;em&gt;jobs.properties.atc.basic_auth_password&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;click the &amp;ldquo;&amp;equiv;&amp;rdquo; (hamburger) in the upper left hand corner (yes, again)&lt;/li&gt;
&lt;li&gt;click the &amp;ldquo;&amp;#x25b6;&amp;rdquo; (play button) that appears below the hamburger.
The banner at the top of the screen will switch from light-blue to black.
The page should look like this:&lt;/li&gt;
&lt;/ul&gt;



  &lt;figure class=&#34;img-responsive&#34;&gt;
      
          &lt;img src=&#34;/images/concourse-000/unpaused_pipeline.png&#34;  class=&#34;img-responsive&#34; /&gt;
      
      
  &lt;/figure&gt;




&lt;h3 id=&#34;0-9-our-first-integration-test-hello-world:250d623a31e5e446b416d6f4fb2feaa8&#34;&gt;0.9 Our First Integration Test: Hello World&lt;/h3&gt;

&lt;p&gt;We kick off our job:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;click the &lt;em&gt;hello-world&lt;/em&gt; rectangle in the middle of the screen.&lt;/li&gt;
&lt;li&gt;click the &amp;ldquo;&lt;strong&gt;&amp;oplus;&lt;/strong&gt;&amp;rdquo; button in the upper right hand side of the screen&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We see that the  job completes successfully by the
pea-green color. We click &amp;ldquo;&lt;strong&gt;&amp;gt;_ say-hello&lt;/strong&gt;&amp;rdquo; to see the output:&lt;/p&gt;



  &lt;figure class=&#34;img-responsive&#34;&gt;
      
          &lt;img src=&#34;/images/concourse-000/success.png&#34;  class=&#34;img-responsive&#34; /&gt;
      
      
  &lt;/figure&gt;




&lt;h2 id=&#34;1-0-conclusion:250d623a31e5e446b416d6f4fb2feaa8&#34;&gt;1.0 Conclusion&lt;/h2&gt;

&lt;p&gt;We have demonstrated with the ease with which one can deploy a CI server
using a combination of &lt;em&gt;Concourse&lt;/em&gt; and &lt;em&gt;bosh-init&lt;/em&gt;, a deployment which takes
less than a quarter hour from start (no disk, no OS) to finish (a publicly-accessible,
up-and-running CI server) and which is easily re-deployed.&lt;/p&gt;

&lt;p&gt;We recognize that our deployment is incomplete, that it lacks the workers
necessary to run jobs of any consequence.  We will describe how to manually
provision workers in our next blog post.&lt;/p&gt;

&lt;p&gt;One of the benefits of the &lt;em&gt;Concourse/bosh-init&lt;/em&gt; combination is that &lt;em&gt;Concourse&lt;/em&gt;
stores its state on a &lt;a href=&#34;https://bosh.io/docs/persistent-disks.html&#34;&gt;persistent disk&lt;/a&gt;,
so that re-deploying the CI server (e.g. new OS, new Concourse) won&amp;rsquo;t cause the loss
of the pipeline configuration or build history.&lt;/p&gt;

&lt;h2 id=&#34;appendix-a-concourse-yearly-costs-80-34:250d623a31e5e446b416d6f4fb2feaa8&#34;&gt;Appendix A. Concourse Yearly Costs: $80.34&lt;/h2&gt;

&lt;p&gt;The yearly cost of running a Concourse server is $80.34. Note that this
does not include the cost of the worker.
Had we chosen to implement the recommended m3.large EC2 instance for a worker, it
would have increased our yearly cost by $713.54 &lt;sup&gt;&lt;a href=&#34;#m3.large&#34;&gt;[m3.large]&lt;/a&gt;&lt;/sup&gt; .&lt;/p&gt;

&lt;p&gt;Here are our costs:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Expense&lt;/th&gt;
&lt;th&gt;Vendor&lt;/th&gt;
&lt;th&gt;Cost&lt;/th&gt;
&lt;th&gt;Cost / year&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;ci.blabbertabber.com&lt;/em&gt; cert&lt;/td&gt;
&lt;td&gt;cheapsslshop.com&lt;/td&gt;
&lt;td&gt;$14.85 3-year  &lt;sup&gt;&lt;a href=&#34;#inexpensive-SSL&#34;&gt;[inexpensive-SSL]&lt;/a&gt;&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;$4.95&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;EC2 t2.micro instance&lt;/td&gt;
&lt;td&gt;Amazon AWS&lt;/td&gt;
&lt;td&gt;$0.0086 / hour &lt;sup&gt;&lt;a href=&#34;#t2.micro&#34;&gt;[t2.micro]&lt;/a&gt;&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;$75.39&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;footnotes:250d623a31e5e446b416d6f4fb2feaa8&#34;&gt;Footnotes&lt;/h2&gt;

&lt;p&gt;&lt;a name=&#34;snowflakes&#34;&gt;&lt;sup&gt;[snowflakes]&lt;/sup&gt;&lt;/a&gt;
Even the most innocuous changes to a CI server can be fraught with anxiety: two years
ago when we were migrating one of our development team&amp;rsquo;s Jenkins CI server VM
from one datastore to another (a very low-risk operation),
we needed to have several meetings with the Team&amp;rsquo;s
product manager and the anchor before they were willing to allow us to proceed
with the migration.&lt;/p&gt;

&lt;p&gt;&lt;a name=&#34;aws_tooling&#34;&gt;&lt;sup&gt;[AWS Tooling]&lt;/sup&gt;&lt;/a&gt;
We appreciate that creating the AWS infrastructure (VPC, Elastic IP, Key Pair)
is tedious and a bit of a clickfest. We&amp;rsquo;re working to make this much easier, soon.
Per Rob Dimsdale, &amp;ldquo;the MEGA team is actively working on tooling to improve the
user-experience of creating the AWS stack for Concourse, but we&amp;rsquo;re not ready
for public consumption of that tooling just yet.&amp;rdquo; Stay tuned.&lt;/p&gt;

&lt;p&gt;&lt;a name=&#34;lets_encrypt&#34;&gt;&lt;sup&gt;[Let&amp;rsquo;s Encrypt]&lt;/sup&gt;&lt;/a&gt;
&lt;a href=&#34;https://letsencrypt.org/&#34;&gt;Let&amp;rsquo;s Encrypt&lt;/a&gt; is a &amp;ldquo;free, automated, and open&amp;rdquo;
Certificate Authority which issues valid SSL certificates free of charge.
We are eagerly awaiting its launch, which hopefully will happen within the
next few weeks.&lt;/p&gt;

&lt;p&gt;&lt;a name=&#34;ELB-pricing&#34;&gt;&lt;sup&gt;[ELB-pricing]&lt;/sup&gt;&lt;/a&gt; ELB pricing, as of this writing, is &lt;a href=&#34;https://aws.amazon.com/elasticloadbalancing/pricing/&#34;&gt;$0.025/hour&lt;/a&gt;, $0.60/day, $219.1455 / year (assuming 365.2425 days / year).&lt;/p&gt;

&lt;p&gt;&lt;a name=&#34;m3.large&#34;&gt;&lt;sup&gt;[m3.large]&lt;/sup&gt;&lt;/a&gt; Amazon effectively charges &lt;a href=&#34;https://aws.amazon.com/ec2/pricing/&#34;&gt;$0.0814/hour&lt;/a&gt; for a 1 year term all-upfront m3.large reserved instance.&lt;/p&gt;

&lt;p&gt;&lt;a name=&#34;inexpensive-SSL&#34;&gt;&lt;sup&gt;[inexpensive-SSL]&lt;/sup&gt;&lt;/a&gt; One shouldn&amp;rsquo;t pay more than
$25 for a 3-year certificate. We used &lt;a href=&#34;https://www.cheapsslshop.com/comodo-positive-ssl&#34;&gt;SSLSHOP&lt;/a&gt; to purchase our &lt;em&gt;Comodo Positive SSL&lt;/em&gt;, but there are many good SSL vendors, and we don&amp;rsquo;t endorse one over
the other.&lt;/p&gt;

&lt;p&gt;&lt;a name=&#34;t2.micro&#34;&gt;&lt;sup&gt;[t2.micro]&lt;/sup&gt;&lt;/a&gt; Amazon effectively charges &lt;a href=&#34;https://aws.amazon.com/ec2/pricing/&#34;&gt;$0.0086/hour&lt;/a&gt; for a 1 year term all-upfront t2.micro reserved instance.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Scaling up to 2000 vms with BOSH</title>
      <link>/post/Scaling-up-to-2000-vms-with-BOSH/</link>
      <pubDate>Fri, 16 Oct 2015 15:43:00 -0700</pubDate>
      
      <guid>/post/Scaling-up-to-2000-vms-with-BOSH/</guid>
      <description>

&lt;p&gt;Pivotal&amp;rsquo;s Cloud Ops team provide production operation support for &lt;a href=&#34;https://run.pivotal.io/&#34;&gt;Pivotal Web Services&lt;/a&gt;, which is a public facing PaaS service based on &lt;a href=&#34;https://github.com/cloudfoundry&#34;&gt;cloudfoundry&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Managing such a big cluster (hundreds of vms) is not easy, and here are some of the challenges we are facing from day to day:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Continuous integration: Deploy the latest code to production every few days.&lt;/li&gt;
&lt;li&gt;Scale entire cluster if the capacity is not enough.&lt;/li&gt;
&lt;li&gt;Upgrade the OS images for the whole cluster with no downtime.&lt;/li&gt;
&lt;li&gt;Deploy the whole cluster to some new environment with some version of code.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;and one tool we heavily rely on for managing our deploy is &lt;a href=&#34;http://bosh.io/&#34;&gt;BOSH&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;BOSH provides software release which packages up all related source code, binary assets, configuration etc, In addition to releases BOSH provides a way to capture all Operating System dependencies as one image which is called Stemcell.&lt;/p&gt;

&lt;p&gt;BOSH tool chain provides a centralized server that manages software releases, Operating System images, persistent data, and system configuration. It provides a clear and simple way of operating deployed system.&lt;/p&gt;

&lt;p&gt;(note: If you are not familiar with BOSH, one suggestion is to go through the &lt;a href=&#34;http://mariash.github.io/learn-bosh/#introduction&#34;&gt;BOSH tutorial&lt;/a&gt; )&lt;/p&gt;

&lt;p&gt;it works great for us, yet we would like to know how far we can go, we did a scaling test for BOSH. and it turns out we can scale to up to 2000 vms and we still didn&amp;rsquo;t see the limit of BOSH yet!&lt;/p&gt;

&lt;p&gt;Here is how we roll it:&lt;/p&gt;

&lt;h3 id=&#34;steps:3b4a1d8c96ef33ec087e61551edb91e2&#34;&gt;Steps:&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Setup an AWS account and increase the EC2 instance limit to 2000&lt;/li&gt;
&lt;li&gt;Set up a BOSH with &lt;a href=&#34;https://bosh.io/docs/using-bosh-init.html&#34;&gt;bosh-init&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Deploy a second BOSH cluster with the first BOSH.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note: We put the nats job into another vm, in order to be able to scale it separately,
This is not recommended, we assume we will see the CPU contention but it doesn&amp;rsquo;t happend,
it bring extra complexity without significant benefit, more points of failure in the control plane.&lt;/p&gt;

&lt;p&gt;A deployment manifest snippet as below:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;jobs:
- name: nats
  templates:
  - name: nats
    release: bosh
  instances: 1
  resource_pool: default
  networks:
  - name: default
    default:
    - dns
    - gateway
    static_ips:
    - 10.0.0.8
  properties:
   nats:
    user: &amp;lt;REPLACE WITH NATS USER&amp;gt;
    password: &amp;lt;REPLACE WITH NATS PASSWORD&amp;gt;
    address: 10.0.0.8
    port: 4222
 
- name: bosh
  templates:
  - name: redis
    release: bosh
  - name: powerdns
    release: bosh
  - name: director
    release: bosh
  - name: registry
    release: bosh
  - name: health_monitor
    release: bosh
  instances: 1
  resource_pool: default
  persistent_disk: 20480
  networks:
  - name: default
    default:
    - dns
    - gateway
    static_ips:
    - 10.0.0.7
  - name: vip_network
    static_ips:
    - &amp;lt;REPLACE WITH IPs&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;Create a deployment manifest for &lt;a href=&#34;https://github.com/pivotal-cf-experimental/dummy-boshrelease&#34;&gt;dummy BOSH release&lt;/a&gt;, create a dummy BOSH release and upload to the second BOSH director.&lt;/li&gt;
&lt;li&gt;BOSH deploy, 2000 vms get created and deployed.&lt;/li&gt;
&lt;li&gt;Make some changes to dummy release, create a new version of dummy release and do BOSH deploy again, 2000 vms get updated with the new code.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We do managed to finish the creation of 2000 vms cluster from 0, update and delete it in one day, yet we found a couple of caveats.&lt;/p&gt;

&lt;h3 id=&#34;caveats:3b4a1d8c96ef33ec087e61551edb91e2&#34;&gt;Caveats:&lt;/h3&gt;

&lt;p&gt;First issue we found is AWS doesn&amp;rsquo;t offer us 2000 instances in one AZ(availability zone). we have more than 1000 instances available in one AZ and less instances available in another AZ.&lt;/p&gt;

&lt;p&gt;This one is easily overcome by creating 2 resource pools in BOSH manifest, each resource pool utilize one AZ. Then BOSH will automatically figure out how to place the vms into 2 AZ separately.&lt;/p&gt;

&lt;p&gt;Another difficulty we found in the deploy is it will takes a lot of time if we can only update one vm at a time. Luckily, BOSH has a max_in_flight option which can specify how many vms to update concurrently.&lt;/p&gt;

&lt;p&gt;But we would like to push it higher see how fast we can go, we found another hard limitation in BOSH which most people don&amp;rsquo;t know, which is the max_threads in &lt;a href=&#34;https://github.com/cloudfoundry/bosh/blob/master/release/jobs/director/spec#L73&#34;&gt;BOSH director configuration&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Before we change the max_threads, even if we set max_in_flight to 256, we found only 32 vms was updating at the same time.&lt;/p&gt;

&lt;p&gt;So we try increasing that max_threads to 256, (you need target the first BOSH and change the BOSH manifest and redeploy to make it effect), and we start seeing some error like below:&lt;/p&gt;

&lt;p&gt;Error 100: Request limit exceeded.
AWS::EC2::Errors::RequestLimitExceeded Request limit exceeded&lt;/p&gt;

&lt;p&gt;After talking to Amazon about it, this is a general limitation in AWS EC2 and we can&amp;rsquo;t easily increase it in AWS side.&lt;/p&gt;

&lt;p&gt;So we step back a bit to find out what the biggest parameter we can use on AWS, and 50 seems to be what we can do in Max in our account.&lt;/p&gt;

&lt;p&gt;(BTW: We also submit some feature request to BOSH so it will retry when the limit exceeded,
and the story is now in the &lt;a href=&#34;https://www.pivotaltracker.com/n/projects/956238/stories/103449734&#34;&gt;backlog&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This is not an ideal speed but still something acceptable, after this tuning, we managed to deploy 2000 vms in 2 hours, update 2000 vms in next 2 hours and delete the whole deployment in next 1.5 hour.&lt;/p&gt;

&lt;p&gt;During the whole deploy process, we observed the CPU, Network, Disk utilization in the director VM (we use c3.xlarge) are still pretty moderate, which indicate we still have headrooms, but we don&amp;rsquo;t go further due to time constraints.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
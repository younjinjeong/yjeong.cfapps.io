<!DOCTYPE html>
<html class="bg-dark-10">




<head>
  <meta charset="utf-8">
  <title> Building machine learning models at scale for data parallel problems on Pivotal&#39;s MPP databases &middot; Pivotal Engineering Journal </title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="/pivotal-ui.css">
  <link rel="stylesheet" href="/styleguide.css">
  <link rel="stylesheet" href="/github.css">
  <link rel="stylesheet" href="/local.css">
  <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
  
  <link href="http://feeds.feedburner.com/PivotalEngineeringJournal" rel="alternate" type="application/rss+xml" title="Pivotal Engineering Journal" />
</head>


<body>
  <div class="container">
  <div id="single-banner" class="pbxxl">
    <div class="single-banner-inner"></div>
    <p class="txt-c mvn">
      <a href="/">
        <svg x="0px" y="0px" width="200px" viewBox="0 0 435 180">
          <rect fill="#01786E" width="435" height="180"/>
          <path fill="#FFFFFF" d="M138.9,65h-11V54.4h11V65z M138.9,125.5h-11V72.8h11V125.5z"/>
          <path fill="#FFFFFF" d="M202.2,72.8l-15.3,45.5c-2.6,7.4-7.3,8.4-11.1,8.4c-5.6,0-9-2.6-11-8.4l-12.7-37.8h-5v-7.7h13.2
          l13.1,42.3c0.6,1.8,0.9,2.9,2.5,2.9c1.7,0,2-1.1,2.5-2.9l13.4-42.3H202.2z"/>
          <path fill="#FFFFFF" d="M229.3,72.8c14.2,0,24.1,9.4,24.1,22.8v8.3c0,13.4-9.9,22.8-24.1,22.8c-14.2,0-24.1-9.4-24.1-22.8
          v-8.3C205.1,82.2,215.1,72.8,229.3,72.8 M229.3,118c8.5,0,13.8-6.4,13.8-14.1v-8.3c0-7.7-5.2-14.1-13.8-14.1
          c-9,0-13.8,6.4-13.8,14.1v8.3C215.5,111.6,220.5,118,229.3,118"/>
          <path fill="#FFFFFF" d="M341,74.5c-5.5-1.4-14-2.4-20.3-2.4c-14.4,0-23.4,9.1-23.4,23.7v5.8c0,14.6,8.9,23.9,23.4,23.9
          c0.3,0,2.9,0,4.1-0.1v-8.8c-0.4,0-3.7,0.1-4.1,0.1c-7.8,0-13.1-6-13.1-15v-5.8c0-9,5.3-15,13.1-15c3.5,0,8.9,0.3,10.8,0.7
          l0.6,0.1l0,43.9h11V76.3C343,75.4,343,75,341,74.5"/>
          <rect x="353.6" y="54.4" fill="#FFFFFF" width="11" height="71"/>
          <path fill="#FFFFFF" d="M89.7,54.4H70.5v71h11.4V64.3h6.7c1.4,0,2.6,0.1,3.8,0.1c9.9,0.2,14.7,4.1,14.7,11.8
          c0,0.3,0,0.5,0,0.8c0,7.1-3.9,11.8-14.7,11.8c-1,0-2.1,0-3.2,0c0,2.7,0,7.8,0,9.5c1.1,0.1,2.2,0.1,3.2,0.1
          c15.5,0,26.4-6.1,26.4-21.4c0-0.3,0-0.6,0-0.9C118.9,60.4,107,54.4,89.7,54.4"/>
          <path fill="#FFFFFF" d="M272.8,61.5v11.3h17.9v8.5h-17.9V112c0,4.8,3.1,4.9,7.5,4.9h10.4v8.5h-14c-10.4,0-15-4.1-15-13.5V63
          L272.8,61.5z"/>
          <path fill="#FFFFFF" d="M369.6,120.9c0-2.5,2-4.6,4.6-4.6c2.5,0,4.6,2.1,4.6,4.6c0,2.5-2,4.6-4.6,4.6
          C371.6,125.5,369.6,123.4,369.6,120.9z M374.2,116.9c-2.2,0-3.9,1.8-3.9,3.9c0,2.1,1.7,3.9,3.9,3.9c2.2,0,3.9-1.8,3.9-3.9
          C378.1,118.7,376.3,116.9,374.2,116.9z M373.2,123.3h-0.5v-5c0,0,1.4,0,1.4,0c1.3,0,1.9,0.6,1.9,1.5c0,0.7-0.4,1.2-1,1.4l1.3,2.1
          h-0.6l-1.2-1.9l-1.2,0.1V123.3z M374.1,120.9c0.8,0,1.3-0.4,1.3-1.1c0-0.7-0.4-1.1-1.4-1.1c0,0-0.8,0-0.8,0v2.2L374.1,120.9z"/>
        </svg>
      </a>
    </p>
    <div class="container txt-c mvxl">
      <h1 class="title type-dark-9 mvn" id="main-title"><a href="/" class="type-dark-11 em-low title">Pivotal Engineering Journal</a></h1>
      <h3 class="type-dark-9 mvn">Technical articles from Pivotal engineers.</h3>
      
    </div>
  </div>
</div>

  <div class="container">
    <div id="post" class="bg-neutral-11 pbxxxl">
      <div class="post-header">
        <div class="phxxl pvl">
          <h1 class="title em-low type-dark-1 mvn">
            
            Building machine learning models at scale for data parallel problems on Pivotal&#39;s MPP databases
          </h1>
          <h2 class="h3 type-dark-3 em-default mvn post-summary">Building machine learning models (ex: scikit-learn) at scale for data parallel problems on Pivotal&rsquo;s MPP databases (Greenplum/HAWQ).</h2>
          <div class="type-dark-5 em-default">
            Posted on
            <span class="post-date"> Sun, Mar 20, 2016 </span>
            
            
            by
            <ul class="authors">
            
              
                <li>
                  
                    <a href="https://twitter.com/being_bayesian">Srivatsan Ramanujam</a>
                  
                </li>
              
            
            </ul>
            <br>
            
              Categories: &nbsp;
                
                  <a href="/categories/data-science">Data Science</a> &nbsp;&nbsp;
                
                  <a href="/categories/greenplum">Greenplum</a> &nbsp;&nbsp;
                
                  <a href="/categories/procedural-languages">Procedural Languages</a> &nbsp;&nbsp;
                
                  <a href="/categories/python">Python</a> &nbsp;&nbsp;
                
                <br/>
              </span>
            
            <a href="https://github.com/pivotal-cf/blog/edit/master/content/post/running-sklearn-models-at-scale-on-mpp.md" class="type-sm">Edit this post on GitHub.</a>
            <div class="pull-right">
              <a href="https://twitter.com/share" class="twitter-share-button"{count} data-via="being_bayesian ">Tweet</a>
              <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
            </div>
          </div>
          <hr />
        </div>
      </div>
      <div class="panel-body phxxl pvn">
        

<p>This is joint work with Heikki Linnakangas and Ivan Novick of Pivotal.</p>

<h2 id="in-database-machine-learning-at-scale-on-mpp-databases:436c2189cb909e233e0a25ef849eb12d">In-database machine learning at scale on MPP databases</h2>

<p><a href="http://greenplum.org/">Greenplum</a> and <a href="http://hawq.incubator.apache.org/">HAWQ</a> are Pivotal&rsquo;s MPP databases with strong analytical capabilities that make them well suited for data science problems at massive scale.  Using in-database machine learning libraries like <a href="http://madlib.net">MADlib</a> and procedural languages like PL/Python and PL/R which enable data scientists to harness the vast ecosystem of machine learning libraries in Python and R, data scientists can build models on massive datasets. In parallelizing machine learning models, there are two flavors of problems that data scientists encounter:</p>

<ol>
<li>Embarassingly parallel problems or data parallel problems.</li>
<li>Model parallel problems and task parallelism</li>
</ol>

<p><a href="https://en.wikipedia.org/wiki/Data_parallelism">Data-parallel problems</a> are those that typically involve building the same machine learning model on different subsets of the full-dataset or for instance, running a grid-search on model parameters, using the same input dataset. These are relatively easy to parallelize on Greenplum and other MPP databases like <a href="http://hawq.incubator.apache.org/">HAWQ</a> using User Defined Functions (UDFs) in PL/Python, PL/R or any other procedural languages supported by these platforms. More information on this can be found here: <a href="http://www.slideshare.net/SrivatsanRamanujam/all-thingspythonpivotal">All things Python @ Pivotal</a> (slides 22-30) and here: <a href="https://github.com/vatsan/gp_xgboost_gridsearch">gp-xgboost-gridsearch</a>.</p>

<p>Model parallel problems or <a href="https://en.wikipedia.org/wiki/Task_parallelism">task parallel</a> problems typically involve building a machine learning model on a dataset that cannot fit into memory, on a distributed cluster. The <a href="http://madlib.incubator.apache.org/">MADlib library</a> (slides 31-52 in <a href="http://www.slideshare.net/SrivatsanRamanujam/all-thingspythonpivotal">All things Python @ Pivotal</a>) explicitly parallelize such models by splitting them into sub-tasks that can be simultaneously executed on multiple nodes of a cluster and combining the results from these sub-tasks to fit the original model.</p>

<p><a href="http://madlib.incubator.apache.org/">MADlib</a> has a very rich collection of machine learning algorithms implemented in-database and is highly performant on large scale datasets. Currently incubating under the <a href="http://www.apache.org/">ASF</a>, new modules are being added with every monthly release of MADlib and the algorithmic breadth has been steadily increasing. Often though, data scientists would love to tap into the vast ocean of machine learning models in other open source projects in Python or R. For instance, <a href="http://scikit-learn.org/stable/">scikit-learn</a> is a popular library of machine learning algorithms in Python, likewise, there are too many libraries to name in R. Being able to tap into these libraries when the model of choice is not available in MADlib, would greatly increase the productivity of data scientists. Data scientists and engineers write User Defined Functions (UDFs) in PL/Python or PL/R, which import these third party libraries and invoke them on an inputs (typically a <code>float8[]</code> or a <code>text[]</code>) that&rsquo;s passed to them. We&rsquo;ve previously spoken at length about the power of procedural languages in blogs such as <a href="https://blog.pivotal.io/data-science-pivotal/products/how-to-scale-native-cc-applications-on-pivotals-mpp-platform-edge-detection-example-part-2">1</a>, <a href="https://blog.pivotal.io/data-science-pivotal/products/twitter-nlp-example-how-to-scale-part-of-speech-tagging-with-mpp-part-2">2</a> or meet-up talks such as <a href="https://www.youtube.com/watch?v=bgOftbw8xRk">3</a>.</p>

<p>One limitation while working on data-parallel problems is the <code>max_field_size</code> limit of Greenplum/Postgres which disallows UDFs from accepting inputs that exceed 1 GB (ex: a float8[]). Greenpl/HAWQ like Postgres have a <a href="http://www.postgresql.org/about/">max field size of 1 GB</a>. While your database table itself could be of the order of hundreds of terabytes in size, no single field (cell) can exceed 1 GB in size. Each row of data could be composed of several hundred columns and collectively a row of data could be really large, but no single column&rsquo;s value, in a given row can exceed 1 GB. This of course is present for performance reasons, but this not a tunable configuration setting.</p>

<pre><code>(http://www.postgresql.org/about/)
Limit Value
Maximum Database Size Unlimited
Maximum Table Size  32 TB
Maximum Row Size  1.6 TB
Maximum Field Size  1 GB
Maximum Rows per Table  Unlimited
Maximum Columns per Table 250 - 1600 depending on column types
Maximum Indexes per Table Unlimited
</code></pre>

<p>For data science problems, one may often have to work with datasets that are represented as matrices (or large linear arrays) that may well exceed the max_field_size limit. For instance, in the example here: <a href="https://github.com/vatsan/gp_xgboost_gridsearch">gp_xgboost_gridsearch</a>, several <a href="https://github.com/dmlc/xgboost">XGBoost</a> models are built in parallel for every possible combination of the input parameters. If the input dataset exceeds 1 GB in size, these UDFs would error out due to the violation of the <code>max_fieldsize_limit</code>.  This limitation prevents users from harnessing the full power of the MPP cluster even when segment hosts typically have a lot more memory than the max_field_size limit. Typically, our customers have clusters with anywhere from 4 to 16 nodes (or more), with each node having upto 8 segments. These beefy machines also have a lot of RAM ranging from 64 GB to 256 GB.</p>

<p>Granted, in a multi-user environment, one has to be mindful of not eating into shared RAM to avoid slowing down or preventing other users from executing their queries. However, it would be useful to have the ability to build machine learning models using popular Python &amp; R libraries, that could well exceed the max_fieldsize_limit.</p>

<p>In this blog, we&rsquo;ll demonstrate how to work around the max_fieldsize_limit to write UDFs in PL/Python, that harness popular machine learning libraries like scikit-learn, for training models in parallel, on datasets several tens to hundreds of gigabytes in size.</p>

<h2 id="demonstrating-the-limitation-introduced-by-the-max-field-size-limit-of-1-gb:436c2189cb909e233e0a25ef849eb12d">Demonstrating the limitation introduced by the <code>max_field_size</code> limit of 1 GB</h2>

<h4 id="1-create-a-table-with-rows-containing-a-field-close-to-max-fieldsize-1-gb:436c2189cb909e233e0a25ef849eb12d">1. Create a table with rows containing a field close to max_fieldsize (~ 1 GB)</h4>

<pre><code class="language-python">-- An array of 120000000 float8(8 bytes) types = 960 MB
--1) Define UDF to generate large arrays
create or replace function gen_array(x int)
returns float8[]
as
$$
    from random import random
    return [random() for _ in range(x)]
$$language plpythonu;

--2) Create a table
drop table if exists cellsize_test;
create table cellsize_test
as
(
    select
        1 as row,
        y,
        gen_array(120000000) as arr
    from
        generate_series(1, 3) y
) distributed by (row);
</code></pre>

<h4 id="2-attempt-to-pass-an-input-1-gb-to-a-udf-to-demonstrate-how-it-fails-due-the-violation-of-max-fieldsize-limit:436c2189cb909e233e0a25ef849eb12d">2. Attempt to pass an input &gt; 1 GB to a UDF to demonstrate how it fails due the violation of max_fieldsize_limit</h4>

<p>We first define a User Defined Aggregate (UDA) that concatenates successive rows of data consisting of arrays and returns a large linear array. We could think of this as unstacking a matrix into a collection of row vectors, so that we could pass it into a PL/Python UDF</p>

<pre><code class="language-python">--1) Define a UDA to concatenate arrays
DROP AGGREGATE IF EXISTS array_agg_array(anyarray) CASCADE;
CREATE ORDERED AGGREGATE array_agg_array(anyarray)
(
    SFUNC = array_cat,
    STYPE = anyarray
);

--2) Define a UDF to consume a really large array and return its size
create or replace function consume_large_array(x float8[])
returns text
as
$$
    return 'size of x:{0}'.format(len(x))
$$language plpythonu;

--3) Invoke the UDF &amp; UDA to demonstrate failure due to max_fieldsize_limit
select
    row,
    consume_large_array(arr)
from
(

    select
        row,
        array_agg_array(arr) as arr
    from
        cellsize_test
    group by
        row
)q;
</code></pre>

<p>This results in the following error, which confirms the limitation we previously described.</p>

<pre><code>DatabaseError: Execution failed on sql '
--1) Define a UDA to concatenate arrays
DROP AGGREGATE IF EXISTS array_agg_array(anyarray) CASCADE;
CREATE ORDERED AGGREGATE array_agg_array(anyarray)
(
    SFUNC = array_cat,
    STYPE = anyarray
);


--2) Define a UDF to consume a really large array and return its size
create or replace function consume_large_array(x float8[])
returns text
as
$$
    return 'size of x:{0}'.format(len(x))
$$language plpythonu;

--3) Invoke the UDF &amp; UDA to demonstrate failure due to max_fieldsize_limit
select
    row,
    consume_large_array(arr)
from
(

    select
        row,
        array_agg_array(arr) as arr
    from
        cellsize_test
    group by
        row
)q;': array size exceeds the maximum allowed (134217727)  (seg42 slice1 sdw1:40000 pid=25165)
</code></pre>

<p>Now we will demonstrate how to work-around the max_field_size limit by making using of the static &amp; global dictionaries available in PL/Python and PL/R UDFs.</p>

<h4 id="3-using-the-global-dictionary-gd-demonstrate-how-to-use-a-udf-that-processes-inputs-exceeding-max-field-size:436c2189cb909e233e0a25ef849eb12d">3. Using the global dictionary <code>GD</code>, demonstrate how to use a UDF that processes inputs exceeding max_field_size</h4>

<p>All PL/Python UDFs have two dictionaries, <a href="http://www.postgresql.org/docs/8.2/static/plpython-funcs.html">SD and GD</a>, that can be used to cache data in memory.</p>

<ol>
<li>SD is private to a UDF, it is used to cache data between function calls in a given transaction.</li>
<li>GD is global dictionary, it is available to all UDFs within a transaction.</li>
</ol>

<p>Here&rsquo;s the approach we&rsquo;ll take:</p>

<p><img src="https://raw.githubusercontent.com/pivotal/blog/master/static/images/largescale_sklearn_models_mpp.png" alt="large scale sklearn models on mpp" /></p>

<p>We will work with the <a href="https://archive.ics.uci.edu/ml/datasets/Wine+Quality">Wine Quality</a> dataset from the UCI machine learning repository.
We&rsquo;ve replicated the rows of the dataset several times, to create a database table with cells which well exceed the <code>max_field_size</code> limit.</p>

<pre><code class="language-sql">select
    *
from
    wine_sample
limit 10;
</code></pre>

<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>model</th>
      <th>features</th>
      <th>quality</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2</td>
      <td>[3.0, 13.27, 4.28, 2.26, 20.0, 120.0, 1.59, 0.69, 0.43, 1.35, 10.2, 0.59, 1.56]</td>
      <td>835</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>[2.0, 11.56, 2.05, 3.23, 28.5, 119.0, 3.18, 5.08, 0.47, 1.87, 6.0, 0.93, 3.69]</td>
      <td>465</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2</td>
      <td>[2.0, 11.46, 3.74, 1.82, 19.5, 107.0, 3.18, 2.58, 0.24, 3.58, 2.9, 0.75, 2.81]</td>
      <td>562</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2</td>
      <td>[2.0, 12.37, 1.17, 1.92, 19.6, 78.0, 2.11, 2.0, 0.27, 1.04, 4.68, 1.12, 3.48]</td>
      <td>510</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2</td>
      <td>[1.0, 14.22, 3.99, 2.51, 13.2, 128.0, 3.0, 3.04, 0.2, 2.08, 5.1, 0.89, 3.53]</td>
      <td>760</td>
    </tr>
    <tr>
      <th>5</th>
      <td>2</td>
      <td>[2.0, 12.72, 1.81, 2.2, 18.8, 86.0, 2.2, 2.53, 0.26, 1.77, 3.9, 1.16, 3.14]</td>
      <td>714</td>
    </tr>
    <tr>
      <th>6</th>
      <td>2</td>
      <td>[2.0, 13.11, 1.01, 1.7, 15.0, 78.0, 2.98, 3.18, 0.26, 2.28, 5.3, 1.12, 3.18]</td>
      <td>502</td>
    </tr>
    <tr>
      <th>7</th>
      <td>2</td>
      <td>[2.0, 12.47, 1.52, 2.2, 19.0, 162.0, 2.5, 2.27, 0.32, 3.28, 2.6, 1.16, 2.63]</td>
      <td>937</td>
    </tr>
    <tr>
      <th>8</th>
      <td>2</td>
      <td>[2.0, 12.08, 2.08, 1.7, 17.5, 97.0, 2.23, 2.17, 0.26, 1.4, 3.3, 1.27, 2.96]</td>
      <td>710</td>
    </tr>
    <tr>
      <th>9</th>
      <td>2</td>
      <td>[2.0, 12.33, 0.99, 1.95, 14.8, 136.0, 1.9, 1.85, 0.35, 2.76, 3.4, 1.06, 2.31]</td>
      <td>750</td>
    </tr>
  </tbody>
</table>
</div>

<p>One example of a machine learning model we may wish to build on this dataset could be a regression model to predict the quality of the wine using attributes such as:</p>

<pre><code>1 - fixed acidity 
2 - volatile acidity 
3 - citric acid 
4 - residual sugar 
5 - chlorides 
6 - free sulfur dioxide 
7 - total sulfur dioxide 
8 - density 
9 - pH 
10 - sulphates 
11 - alcohol 
</code></pre>

<p>These form the <code>features</code> column, which is a <code>float8[]</code> in our table listed above. The <code>model</code> column in the sample table above, could for instance correspond to all wines grown in a given state in the US. Perhaps we may be interested in building a model to predict the quality of wine grown in every state. This toy problem illustrates the data-parallel nature of the modeling task. Next we&rsquo;ll define the UDFs, and UDAs to accomplish our goal of training a model from say <a href="scikit-learn.org/">scikit-learn</a> on a dataset well exceeding the <code>max_field_size</code>.</p>

<h4 id="define-udcts-udfs-and-udas-to-accomplish-our-goal:436c2189cb909e233e0a25ef849eb12d">Define, UDCTs, UDFs and UDAs to accomplish our goal</h4>

<pre><code class="language-python">--1) SFUNC: State transition function, part of a User-Defined-Aggregate definition
-- This function will merely stack every row of input, into the GD variable
drop function if exists stack_rows(
    text,
    text[],
    float8[],
    float8
) cascade;
create or replace function stack_rows(
    key text,
    header text[], -- name of the features column and the dependent variable column
    features float8[], -- independent variables (as array)
    label float8 -- dependent variable column
)
returns text
as
$$
    if 'header' not in GD:
        GD['header'] = header
    if not key:
        gd_key = 'stack_rows'
        GD[gd_key] = [[features, label]]
        return gd_key
    else:
        GD[key].append([features, label])
        return key
$$language plpythonu;

--2) Define the User-Defined Aggregate (UDA) consisting of a state-transition function (SFUNC), a state variable and a FINALFUNC (optional)
drop aggregate if exists stack_rows( 
    text[], -- header (feature names)
    float8[], -- features (feature values),
    float8 -- labels
) cascade;
create ordered aggregate stack_rows(
        text[], -- header (feature names)
        float8[], -- features (feature values),
        float8 -- labels
    )
(
    SFUNC = stack_rows,
    STYPE = text -- the key in GD used to hold the data across calls
);

--3) Create a return type for model results
DROP TYPE IF EXISTS host_mdl_coef_intercept CASCADE;
CREATE TYPE host_mdl_coef_intercept
AS
(
    hostname text, -- hostname on which the model was built
    coef float[], -- model coefficients
    intercept float, -- intercepts
    r_square float -- training data fit
);

--4) Define a UDF to run ridge regression by retrieving the data from the key in GD and returning results
drop function if exists run_ridge_regression(text) cascade;
create or replace function run_ridge_regression(key text)
returns host_mdl_coef_intercept
as
$$
    import os
    import numpy as np   
    import pandas as pd
    from sklearn import linear_model
    
    if key in GD:
        df = pd.DataFrame(GD[key], columns=GD['header'])
        mdl = linear_model.Ridge(alpha = .5)
        X = np.mat(df[GD['header'][0]].values.tolist())
        y = np.mat(df[GD['header'][1]].values.tolist()).transpose()
        mdl.fit(X, y)
        result = [
            os.popen('hostname').read().strip(), 
            mdl.coef_[0], 
            mdl.intercept_[0], 
            mdl.score(X, y)
        ]   
        GD[key] = result        
        result = GD[key]
        del GD[key]
        return result
    else:
        plpy.info('returning None')
        return None
$$ language plpythonu;
</code></pre>

<p>As seen above, we first defined a state-transition function, which is one of the building blocks of a User-Defined Aggregate. This function takes a row of input (in this case it is a record consisting of a float8[] and a float8 corresponding to the feature vector and the dependent variable) and stacks in the <code>GD</code> variable, using a user-specified key. This key in <code>GD</code> is accessible by any other UDFs in the same transaction, thus the UDF <code>run_ridge_regression</code> retrieves all the stacked rows from the <code>GD</code> variable, constructs the input matrix required for the <code>ridge regression</code> model of <code>sklearn</code> and returns a set of rows as result, where each row of output corresponds to the hostname on which the model was built, the coefficients of the model, the intercept and the <a href="https://en.wikipedia.org/wiki/Coefficient_of_determination">coefficient of determination</a> of the model on the training dataset. All these building blocks were combined in the definition of the User-Defined Aggregate.</p>

<p>We can invoke our UDAs and UDFs like so:</p>

<pre><code class="language-sql">select
    model,
    (results).*
from
(
    select
        model,
        run_ridge_regression(
            stacked_input_key
        ) as results
    from
    (
        select
            model,
            stack_rows(
                ARRAY['features', 'quality'], --header or names of input fields
                features, -- feature vector input field
                quality -- label column
            ) as stacked_input_key
        from
            wine_sample
        group by
            model
    )q1
)q2;
</code></pre>

<p>In the innermost query, we merely stacked the rows of the input table into the GD variable. By grouping by <code>model</code>, we&rsquo;re building multiple regression models in parallel, without explicitly parallelizing the implementation of the <code>ridge regression</code> model in <code>scikit-learn</code>.</p>

<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>model</th>
      <th>hostname</th>
      <th>coef</th>
      <th>intercept</th>
      <th>r_square</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>10</td>
      <td>sdw13</td>
      <td>[-317.076377948, 62.0500620027, 0.416072749423, 162.375192268, -8.8567342115, 1.55403179538, 56.4122212105, -108.828043759, -208.649699922, 24.895877024, 42.9492673878, 116.389536874, -66.1348558416]</td>
      <td>162.546696</td>
      <td>0.716523</td>
    </tr>
    <tr>
      <th>1</th>
      <td>9</td>
      <td>sdw2</td>
      <td>[-304.109404213, 74.5250737585, -3.43371878258, 139.27084644, -5.72305248882, 0.744595419201, 77.5014353294, -106.588482015, -167.685124503, 90.1516923867, 39.7803072901, 127.645822384, -101.704587526]</td>
      <td>-30.238416</td>
      <td>0.708814</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>sdw2</td>
      <td>[-307.18651323, 61.5224373481, 1.47063776707, 193.259830857, -18.6214527421, 2.4335710989, 91.135375234, -86.3640292245, -153.343722551, 3.05688315498, 39.2513828334, 46.1381861423, -72.2478960996]</td>
      <td>156.581033</td>
      <td>0.776013</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>sdw14</td>
      <td>[-315.19350321, 65.0837954451, -12.3415435935, 151.63900677, -13.7159306955, 1.3239107062, 42.9091133141, -73.3993821817, -50.9281911399, -15.4088436536, 46.4040342628, 83.3349706245, -53.3831134811]</td>
      <td>223.886378</td>
      <td>0.727271</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>sdw9</td>
      <td>[-309.766022096, 100.281441571, -4.41459273681, 133.913228451, -7.24029032969, 1.85096413382, 87.0188975088, -100.394878326, -72.4600856378, 18.50523645, 31.4688276602, 40.273527478, -74.745208132]</td>
      <td>-308.152828</td>
      <td>0.738516</td>
    </tr>
    <tr>
      <th>5</th>
      <td>2</td>
      <td>sdw12</td>
      <td>[-308.143968247, 75.8323235002, -7.24767188617, 153.65120976, -7.21866367193, 2.05021275785, 88.1310253165, -104.709107165, -132.213217375, 41.9375754143, 39.3444712727, 87.2320135084, -83.8829139824]</td>
      <td>-127.344552</td>
      <td>0.720098</td>
    </tr>
    <tr>
      <th>6</th>
      <td>7</td>
      <td>sdw16</td>
      <td>[-292.491831013, 77.9754967544, -2.99699296323, 148.659732944, -9.55581444196, 1.64432942304, 55.6973991837, -84.1137271812, -155.287234458, 17.4856436475, 36.1721932837, 100.274623447, -65.3585353928]</td>
      <td>-69.530631</td>
      <td>0.701506</td>
    </tr>
    <tr>
      <th>7</th>
      <td>8</td>
      <td>sdw6</td>
      <td>[-334.632387783, 51.2647776529, -3.48328272546, 160.491097171, -8.66863247447, 1.08276882394, 33.8001382612, -96.3381754755, -221.760838113, 33.3029881822, 47.5759038967, 129.677807199, -82.2412541428]</td>
      <td>411.147338</td>
      <td>0.701124</td>
    </tr>
    <tr>
      <th>8</th>
      <td>6</td>
      <td>sdw5</td>
      <td>[-347.036682861, 70.7144175077, -4.28001413723, 94.2697061067, -3.8362015069, 1.88077507258, 59.2066215719, -123.776836974, -211.836728045, 19.468199028, 46.5848203245, 96.0841879507, -56.4798790464]</td>
      <td>158.772706</td>
      <td>0.721023</td>
    </tr>
    <tr>
      <th>9</th>
      <td>1</td>
      <td>sdw2</td>
      <td>[-305.207078775, 69.1563749904, 1.85205159406, 142.053062793, -9.84353848356, 2.20689559387, 49.6397176189, -90.0801141803, -217.435154348, 31.7993674214, 42.5733663559, 137.24861027, -75.2972668947]</td>
      <td>21.565687</td>
      <td>0.722828</td>
    </tr>
  </tbody>
</table>
</div>

<p>In the results above, the <code>hostname</code> column tell us in which host of the cluster was the <code>ridge regression</code> model for the corresponding value in the <code>model</code> column was trained on. We can thus confirm that we were able to build <code>scikit-learn</code> models in parallel, on a dataset which exceeded the <code>max_field_size</code> limit. This allows data scientists to harness the amazing ecosystem of machine learning libraries in Python and R, via procedural languages like PL/Python and PL/R (amongst others), at scale for data parallel problems.</p>

<h2 id="what-about-pl-r:436c2189cb909e233e0a25ef849eb12d">What about PL/R?</h2>

<p>Similar functionality is possible in PL/R as well. Refer to the global variables section on the <a href="http://www.joeconway.com/plr/doc/plr-global-data.html">PL/R guide</a> for more details. In short, you can assign values to a global environment variable inside your UDFs like so:</p>

<pre><code>assign(&quot;global_variable_for_your_udf&quot;, matrix ,env=.GlobalEnv)
</code></pre>

<h2 id="more-information:436c2189cb909e233e0a25ef849eb12d">More information</h2>

<p>If you&rsquo;d like to look at the raw <code>Jupyter notebooks</code>, you can clone them from <a href="https://github.com/vatsan/gp-sql-snippets/blob/master/notebooks/01_max_fieldsize_1gb_workaround.ipynb">here</a>.</p>

      </div>
    </div>
  </div>
  <link rel="stylesheet" href="/css/highlight-github.css">
<script src="/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>

</body>
</html>
